{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de176770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.stats import mannwhitneyu\n",
    "import gc\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\User\\\\Downloads\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af06fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f\"{data_path}2019-Oct.csv\", usecols=['event_time', 'event_type', 'product_id', 'brand', 'category_code', 'price', 'user_id', 'user_session'])\n",
    "df2 = pd.read_csv(f\"{data_path}2019-Nov.csv\", usecols=['event_time', 'event_type', 'product_id', 'brand', 'category_code', 'price', 'user_id', 'user_session'])\n",
    "df = pd.concat([df1, df2])\n",
    "del df1, df2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ecb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_code'] = df['category_code'].astype('category')\n",
    "df['category_code'] = df['category_code'].cat.add_categories('unknown.')\n",
    "df['category_code'].fillna('unknown.', inplace=True)\n",
    "\n",
    "df['general_category'] = df['category_code'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_session'] = df['user_session'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b782293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime 처리\n",
    "df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "df['date'] = df['event_time'].dt.date\n",
    "\n",
    "# 사용자별, 시간순 정렬\n",
    "df = df.sort_values(['user_id', 'event_time'])\n",
    "\n",
    "# 구매 발생 플래그\n",
    "df['is_purchase'] = df['event_type'] == 'purchase'\n",
    "\n",
    "# 하루 단위 기준 세션 초기화\n",
    "df['daily_session'] = df['user_id'].astype(str) + '_' + df['date'].astype(str)\n",
    "\n",
    "# 구매 발생 시 세션 쪼개기: 구매 이후는 새로운 세션으로\n",
    "def split_on_purchase(group):\n",
    "    session_id = []\n",
    "    counter = 0\n",
    "    for is_p in group['is_purchase']:\n",
    "        session_id.append(counter)\n",
    "        if is_p:\n",
    "            counter += 1\n",
    "    return session_id\n",
    "\n",
    "# 사용자 + 날짜별로 쪼개서 세션 나누기\n",
    "df['session_split_id'] = df.groupby(['user_id', 'date']).apply(split_on_purchase).explode().values\n",
    "\n",
    "df['custom_session'] = (\n",
    "    df['user_id'].astype(str) + '_' +\n",
    "    df['date'].astype(str) + '_' +\n",
    "    df['session_split_id'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_col : ['user_id', 'event_time', 'event_type', 'product_id', 'brand', 'category_code', 'price', 'user_session', 'is_purchase', 'daily_session', 'custom_session']\n",
    "df.drop(columns=['user_session', 'is_purchase', 'daily_session', 'session_split_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451980b",
   "metadata": {},
   "source": [
    "### 0/1 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e384982",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_sessions = df[df['event_type'] == 'purchase']['custom_session'].unique()\n",
    "df['buy'] = df['custom_session'].isin(buy_sessions).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy = df[df['buy'] == 1]\n",
    "df = df[df['buy'] == 0].sample(n=int(len(buy)), random_state=42)\n",
    "df = pd.concat([buy, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf07bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del buy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3385caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_category = df.groupby('general_category')['price'].median().to_dict()\n",
    "df['relative_price'] = df['price'] / df['general_category'].map(median_price_category)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "del median_price_category\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['custom_session'] = df['custom_session'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003a3c4",
   "metadata": {},
   "source": [
    "### first save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd162089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{data_path}custom_session_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1062e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{data_path}custom_session_sampled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669a7af",
   "metadata": {},
   "source": [
    "### make log_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dictionary(df):\n",
    "    df = df.sort_values(['user_id', 'event_time'])\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "\n",
    "    session_data = []\n",
    "\n",
    "    for session, group in df.groupby('custom_session'):\n",
    "        group = group.sort_values('event_time')\n",
    "\n",
    "        times = group['event_time'].tolist()\n",
    "        time_since_start = [(t - times[0]).total_seconds() for t in times]\n",
    "        time_since_prev = [0] + [(times[i] - times[i-1]).total_seconds() for i in range(1, len(times))]\n",
    "\n",
    "        events = []\n",
    "        for _, row in group.iterrows():\n",
    "            if row['event_type'] == 'purchase':\n",
    "                continue  # 구매 제외\n",
    "            \n",
    "            events.append({\n",
    "                \"event_type\": row[\"event_type\"],\n",
    "                \"product_id\": row[\"product_id\"],\n",
    "                \"category_code\": row[\"category_code\"] if pd.notna(row[\"category_code\"]) else \"unknown.\",\n",
    "                \"general_category\": row[\"general_category\"] if pd.notna(row[\"general_category\"]) else \"unknown\",\n",
    "                \"brand\": row[\"brand\"] if pd.notna(row[\"brand\"]) else \"unknown\",\n",
    "                \"price\": row[\"price\"],\n",
    "                \"relative_price\": row[\"relative_price\"],\n",
    "                \"time_since_start\": time_since_start[len(events)],\n",
    "                \"time_since_prev\": time_since_prev[len(events)]\n",
    "            })\n",
    "\n",
    "        session_data.append({\n",
    "            \"session\": session,\n",
    "            \"event\": events,\n",
    "            \"buy\": int(group[\"buy\"].max())\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(session_data)\n",
    "\n",
    "df = log_dictionary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa910dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_code(events):\n",
    "    for event in events:\n",
    "        if event['category_code'] == 'unknown.':\n",
    "            event['category_code'] = 'unknown'\n",
    "    return events\n",
    "\n",
    "df['event'] = df['event'].apply(category_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_event = LabelEncoder()\n",
    "unique_events = ['view', 'cart', 'purchase']\n",
    "le_event.fit(unique_events)\n",
    "le_event.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_event_type(events):\n",
    "    for event in events:\n",
    "        event['event_type'] = le_event.transform([event['event_type']])[0]\n",
    "    return events\n",
    "\n",
    "df['event'] = df['event'].apply(encode_event_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e4309",
   "metadata": {},
   "source": [
    "### second save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{data_path}log_dict_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{data_path}log_dict_sampled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256188bc",
   "metadata": {},
   "source": [
    "### 전체 파이프라인: 데이터 파싱 -> vocabs -> tf.data -> Transformer 모델 -> 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06210f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sessions: 3319576\n",
      "example session events: [{'event_type': 2, 'product_id': 17301868, 'category_code': 'unknown', 'general_category': 'unknown', 'brand': 'sheba', 'price': 144.14, 'relative_price': 1.78922542204568, 'time_since_start': 0.0, 'time_since_prev': 0}, {'event_type': 2, 'product_id': 17300771, 'category_code': 'unknown', 'general_category': 'unknown', 'brand': 'bulgari', 'price': 57.72, 'relative_price': 0.7164846077457795, 'time_since_start': 66.0, 'time_since_prev': 66.0}, {'event_type': 2, 'product_id': 17300789, 'category_code': 'unknown', 'general_category': 'unknown', 'brand': 'bulgari', 'price': 55.79, 'relative_price': 0.6925273088381331, 'time_since_start': 81.0, 'time_since_prev': 15.0}, {'event_type': 2, 'product_id': 17301041, 'category_code': 'unknown', 'general_category': 'unknown', 'brand': 'shiseido', 'price': 93.5, 'relative_price': 1.160625620655412, 'time_since_start': 134.0, 'time_since_prev': 53.0}]\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data and parse event strings to list of dicts\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\custom_session_sample_oct_nov_log_dict_encoded_sampled.csv\")\n",
    "df['event'] = df['event'].apply(eval)\n",
    "\n",
    "def parse_events(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return json.loads(x)\n",
    "            except Exception:\n",
    "                return []\n",
    "    elif isinstance(x, list):\n",
    "        return x\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "df['events_parsed'] = df['event'].apply(parse_events)\n",
    "\n",
    "print(\"n_sessions:\", len(df))\n",
    "print(\"example session events:\", df['events_parsed'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35120d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Split train/val stratified\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['buy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435ad149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_type vocab_size: 3\n",
      "category_code vocab_size: 131\n",
      "general_category vocab_size: 15\n",
      "brand vocab_size: 4068\n"
     ]
    }
   ],
   "source": [
    "# 3) Build vocabs on train_df\n",
    "cat_fields = ['event_type', 'category_code', 'general_category', 'brand']\n",
    "num_fields = ['price', 'relative_price', 'time_since_start', 'time_since_prev']\n",
    "VOCAB_SIZES = {\n",
    "    'event_type': None,\n",
    "    'category_code': 200,\n",
    "    'general_category': None,\n",
    "    'brand': 5000\n",
    "}\n",
    "\n",
    "counters = {f: Counter() for f in cat_fields}\n",
    "for ev_list in train_df['events_parsed']:\n",
    "    for ev in ev_list:\n",
    "        for f in cat_fields:\n",
    "            val = ev.get(f, 'unknown')\n",
    "            if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "                val = 'unknown'\n",
    "            counters[f][str(val)] += 1\n",
    "\n",
    "def build_vocab(counter, max_size=None, min_freq=1, add_oov=True):\n",
    "    items = [k for k,v in counter.items() if v >= min_freq]\n",
    "    items_sorted = sorted(items, key=lambda x: counter[x], reverse=True)\n",
    "    if max_size:\n",
    "        items_sorted = items_sorted[:max_size]\n",
    "    vocab = {v: i+1 for i, v in enumerate(items_sorted)}  # reserve 0 for PAD\n",
    "    if add_oov:\n",
    "        vocab['<OOV>'] = len(vocab) + 1\n",
    "    return vocab\n",
    "\n",
    "vocab = {}\n",
    "for f in cat_fields:\n",
    "    vocab[f] = build_vocab(counters[f], max_size=VOCAB_SIZES.get(f, None))\n",
    "    print(f, 'vocab_size:', len(vocab[f]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152f751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) fit scalers on train_df\n",
    "all_nums_train = {f: [] for f in num_fields}\n",
    "for ev_list in train_df['events_parsed']:\n",
    "    for ev in ev_list:\n",
    "        for nf in num_fields:\n",
    "            val = ev.get(nf, 0.0)\n",
    "            if val is None:\n",
    "                val = 0.0\n",
    "            all_nums_train[nf].append(float(val))\n",
    "\n",
    "scalers = {}\n",
    "for nf in num_fields:\n",
    "    scaler = StandardScaler()\n",
    "    vals = np.array(all_nums_train[nf]).reshape(-1,1)\n",
    "    scaler.fit(vals)\n",
    "    scalers[nf] = scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6567fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) session_to_features\n",
    "MAX_SEQ_LEN = 32\n",
    "\n",
    "def map_cat(value, vocab_map):\n",
    "    if value is None:\n",
    "        return vocab_map.get('<OOV>', 1)\n",
    "    s = str(value)\n",
    "    if s in vocab_map:\n",
    "        return vocab_map[s]\n",
    "    else:\n",
    "        return vocab_map.get('<OOV>', 1)\n",
    "\n",
    "def session_to_features(ev_list):\n",
    "    seq_len = min(len(ev_list), MAX_SEQ_LEN)\n",
    "    cat_indices = {f: np.zeros(MAX_SEQ_LEN, dtype=np.int32) for f in cat_fields}\n",
    "    num_values = np.zeros((MAX_SEQ_LEN, len(num_fields)), dtype=np.float32)\n",
    "    mask = np.zeros((MAX_SEQ_LEN,), dtype=np.float32)\n",
    "    for i in range(seq_len):\n",
    "        ev = ev_list[-seq_len + i]\n",
    "        mask[i] = 1.0\n",
    "        for f in cat_fields:\n",
    "            cat_indices[f][i] = map_cat(ev.get(f, 'unknown'), vocab[f])\n",
    "        for j, nf in enumerate(num_fields):\n",
    "            val = ev.get(nf, 0.0)\n",
    "            if val is None:\n",
    "                val = 0.0\n",
    "            scaled = scalers[nf].transform(np.array([[float(val)]]))[0,0]\n",
    "            num_values[i, j] = scaled\n",
    "    return cat_indices, num_values, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16095e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing 2987618 sessions...\n",
      "Saved preprocessed data to train_preprocessed.npz\n",
      "Start preprocessing 331958 sessions...\n",
      "Saved preprocessed data to val_preprocessed.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scalers.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) transform and save (.npz)\n",
    "def preprocess_and_save(df_input, filename):\n",
    "    all_cat = {f: [] for f in cat_fields}\n",
    "    all_num = []\n",
    "    all_mask = []\n",
    "    all_labels = []\n",
    "    print(f\"Start preprocessing {len(df_input)} sessions...\")\n",
    "    for i, row in df_input.iterrows():\n",
    "        cat_idx, num_vals, mask = session_to_features(row['events_parsed'])\n",
    "        for f in cat_fields:\n",
    "            all_cat[f].append(cat_idx[f])\n",
    "        all_num.append(num_vals)\n",
    "        all_mask.append(mask)\n",
    "        all_labels.append(row['buy'])\n",
    "\n",
    "    # to numpy\n",
    "    for f in cat_fields:\n",
    "        all_cat[f] = np.array(all_cat[f], dtype=np.int32)\n",
    "    all_num = np.array(all_num, dtype=np.float32)\n",
    "    all_mask = np.array(all_mask, dtype=np.float32)\n",
    "    all_labels = np.array(all_labels, dtype=np.float32)\n",
    "    np.savez_compressed(filename, **all_cat, numeric=all_num, mask=all_mask, labels=all_labels)\n",
    "    print(f\"Saved preprocessed data to {filename}\")\n",
    "\n",
    "# preprocess_and_save(train_df, \"train_preprocessed.npz\")\n",
    "# preprocess_and_save(val_df, \"val_preprocessed.npz\")\n",
    "\n",
    "# # save vocab\n",
    "# with open('vocab.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # save scaler\n",
    "# joblib.dump(scalers, 'scalers.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce7102",
   "metadata": {},
   "source": [
    "# Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62519302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.stats import mannwhitneyu\n",
    "import gc\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27fe4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Load preprocessed data and make tf.data.Dataset\n",
    "cat_fields = ['event_type', 'category_code', 'general_category', 'brand']\n",
    "num_fields = ['price', 'relative_price', 'time_since_start', 'time_since_prev']\n",
    "MAX_SEQ_LEN = 32\n",
    "\n",
    "# load vocab \n",
    "with open('vocab.json', 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# load scaler\n",
    "scalers = joblib.load('scalers.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f8ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_preprocessed_npz(filename):\n",
    "    data = np.load(filename)\n",
    "    features = {f: data[f] for f in cat_fields}\n",
    "    features['numeric'] = data['numeric']\n",
    "    features['mask'] = data['mask']\n",
    "    labels = data['labels']\n",
    "    return features, labels\n",
    "\n",
    "def make_tf_dataset(features, labels, batch_size=512, shuffle=True):\n",
    "    def gen():\n",
    "        for i in range(len(labels)):\n",
    "            feat = {f: features[f][i] for f in cat_fields}\n",
    "            feat['numeric'] = features['numeric'][i]\n",
    "            feat['mask'] = features['mask'][i]\n",
    "            label = np.array([labels[i]], dtype=np.float32)\n",
    "            yield feat, label\n",
    "    output_signature = (\n",
    "        {**{f: tf.TensorSpec(shape=(MAX_SEQ_LEN,), dtype=tf.int32) for f in cat_fields},\n",
    "         'numeric': tf.TensorSpec(shape=(MAX_SEQ_LEN, len(num_fields)), dtype=tf.float32),\n",
    "         'mask': tf.TensorSpec(shape=(MAX_SEQ_LEN,), dtype=tf.float32)},\n",
    "        tf.TensorSpec(shape=(1,), dtype=tf.float32)\n",
    "    )\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=2048)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_features, train_labels = load_preprocessed_npz(\"train_preprocessed.npz\")\n",
    "val_features, val_labels = load_preprocessed_npz(\"val_preprocessed.npz\")\n",
    "\n",
    "train_ds = make_tf_dataset(train_features, train_labels, shuffle=True)\n",
    "val_ds = make_tf_dataset(val_features, val_labels, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d402d5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=({'event_type': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'category_code': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'general_category': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'brand': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'numeric': TensorSpec(shape=(None, 32, 4), dtype=tf.float32, name=None), 'mask': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)}, TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>,\n",
       " <PrefetchDataset element_spec=({'event_type': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'category_code': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'general_category': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'brand': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), 'numeric': TensorSpec(shape=(None, 32, 4), dtype=tf.float32, name=None), 'mask': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)}, TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e70c626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " event_type (InputLayer)        [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " category_code (InputLayer)     [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " general_category (InputLayer)  [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " brand (InputLayer)             [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " numeric (InputLayer)           [(None, 32, 4)]      0           []                               \n",
      "                                                                                                  \n",
      " emb_event_type (Embedding)     (None, 32, 64)       256         ['event_type[0][0]']             \n",
      "                                                                                                  \n",
      " emb_category_code (Embedding)  (None, 32, 64)       8448        ['category_code[0][0]']          \n",
      "                                                                                                  \n",
      " emb_general_category (Embeddin  (None, 32, 64)      1024        ['general_category[0][0]']       \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " emb_brand (Embedding)          (None, 32, 64)       260416      ['brand[0][0]']                  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32, 64)       320         ['numeric[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 64)       0           ['emb_event_type[0][0]',         \n",
      "                                                                  'emb_category_code[0][0]',      \n",
      "                                                                  'emb_general_category[0][0]',   \n",
      "                                                                  'emb_brand[0][0]',              \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 32, 64)      0           ['add[0][0]']                    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " mask (InputLayer)              [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_block (Tra  (None, 32, 64)      99712       ['tf.__operators__.add[0][0]',   \n",
      " nsformerEncoderBlock)                                            'mask[0][0]']                   \n",
      "                                                                                                  \n",
      " transformer_encoder_block_1 (T  (None, 32, 64)      99712       ['transformer_encoder_block[0][0]\n",
      " ransformerEncoderBlock)                                         ',                               \n",
      "                                                                  'mask[0][0]']                   \n",
      "                                                                                                  \n",
      " transformer_encoder_block_2 (T  (None, 32, 64)      99712       ['transformer_encoder_block_1[0][\n",
      " ransformerEncoderBlock)                                         0]',                             \n",
      "                                                                  'mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 32, 1)        0           ['mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 32, 64)       0           ['transformer_encoder_block_2[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None, 1)           0           ['tf.expand_dims[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None, 64)          0           ['tf.math.multiply[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 1)           0           ['tf.math.reduce_sum_1[0][0]']   \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 64)           0           ['tf.math.reduce_sum[0][0]',     \n",
      "                                                                  'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           4160        ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64)           0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " buy_prob (Dense)               (None, 1)            65          ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 573,825\n",
      "Trainable params: 573,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 8) Model building\n",
    "EMBED_DIM = 64\n",
    "num_heads = 4\n",
    "mlp_dim = EMBED_DIM * num_heads\n",
    "cat_emb_layers = {}\n",
    "for f in cat_fields:\n",
    "    vocab_size = len(vocab[f]) + 1\n",
    "    cat_emb_layers[f] = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIM, mask_zero=True, name=f'emb_{f}')\n",
    "\n",
    "pos_emb = tf.keras.layers.Embedding(input_dim=MAX_SEQ_LEN, output_dim=EMBED_DIM, name='pos_emb')\n",
    "\n",
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads, mlp_dim, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(dim),\n",
    "        ])\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        attn_mask = None\n",
    "        if mask is not None:\n",
    "            attn_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.bool)\n",
    "        attn_out = self.mha(x, x, attention_mask=attn_mask)\n",
    "        attn_out = self.dropout1(attn_out, training=training)\n",
    "        out1 = self.norm1(x + attn_out)\n",
    "        ff = self.ffn(out1)\n",
    "        ff = self.dropout2(ff, training=training)\n",
    "        out2 = self.norm2(out1 + ff)\n",
    "        return out2\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"mlp_dim\": self.mlp_dim,\n",
    "            \"dropout\": self.dropout,\n",
    "        })\n",
    "        return config \n",
    "\n",
    "def build_model():\n",
    "    inputs = {f: tf.keras.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=f) for f in cat_fields}\n",
    "    inputs['numeric'] = tf.keras.Input(shape=(MAX_SEQ_LEN, len(num_fields)), dtype=tf.float32, name='numeric')\n",
    "    inputs['mask'] = tf.keras.Input(shape=(MAX_SEQ_LEN,), dtype=tf.float32, name='mask')\n",
    "\n",
    "    emb_list = []\n",
    "    for f in cat_fields:\n",
    "        e = cat_emb_layers[f](inputs[f])\n",
    "        emb_list.append(e)\n",
    "    numeric_proj = tf.keras.layers.Dense(EMBED_DIM)(inputs['numeric'])\n",
    "    emb_list.append(numeric_proj)\n",
    "\n",
    "    x = tf.keras.layers.Add()(emb_list)\n",
    "    positions = tf.range(start=0, limit=MAX_SEQ_LEN, delta=1)\n",
    "    pos_embeddings = pos_emb(positions)\n",
    "    x = x + pos_embeddings\n",
    "\n",
    "    mask = inputs['mask']\n",
    "    for _ in range(3):\n",
    "        x = TransformerEncoderBlock(dim=EMBED_DIM, num_heads=num_heads, mlp_dim=mlp_dim)(x, mask=mask)\n",
    "\n",
    "    mask_expanded = tf.expand_dims(mask, axis=-1)\n",
    "    x_masked = x * mask_expanded\n",
    "    sum_x = tf.reduce_sum(x_masked, axis=1)\n",
    "    lengths = tf.reduce_sum(mask_expanded, axis=1)\n",
    "    pooled = sum_x / (lengths + 1e-6)\n",
    "\n",
    "    h = tf.keras.layers.Dense(64, activation='relu')(pooled)\n",
    "    h = tf.keras.layers.Dropout(0.3)(h)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid', name='buy_prob')(h)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.BinaryAccuracy(name='acc')])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac7240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5836/5836 [==============================] - 339s 58ms/step - loss: 0.3561 - auc: 0.9149 - acc: 0.8578 - val_loss: 0.2979 - val_auc: 0.9376 - val_acc: 0.8912\n",
      "Epoch 2/10\n",
      "5836/5836 [==============================] - 351s 60ms/step - loss: 0.2850 - auc: 0.9382 - acc: 0.8981 - val_loss: 0.2671 - val_auc: 0.9448 - val_acc: 0.9059\n",
      "Epoch 3/10\n",
      "5836/5836 [==============================] - 345s 59ms/step - loss: 0.2675 - auc: 0.9428 - acc: 0.9064 - val_loss: 0.2613 - val_auc: 0.9466 - val_acc: 0.9084\n",
      "Epoch 4/10\n",
      "5836/5836 [==============================] - 340s 58ms/step - loss: 0.2593 - auc: 0.9450 - acc: 0.9099 - val_loss: 0.2541 - val_auc: 0.9479 - val_acc: 0.9110\n",
      "Epoch 5/10\n",
      "5836/5836 [==============================] - 334s 57ms/step - loss: 0.2537 - auc: 0.9466 - acc: 0.9121 - val_loss: 0.2495 - val_auc: 0.9494 - val_acc: 0.9129\n",
      "Epoch 6/10\n",
      "5836/5836 [==============================] - 323s 55ms/step - loss: 0.2498 - auc: 0.9478 - acc: 0.9137 - val_loss: 0.2461 - val_auc: 0.9502 - val_acc: 0.9145\n",
      "Epoch 7/10\n",
      "5836/5836 [==============================] - 345s 59ms/step - loss: 0.2464 - auc: 0.9489 - acc: 0.9150 - val_loss: 0.2452 - val_auc: 0.9507 - val_acc: 0.9148\n",
      "Epoch 8/10\n",
      "5836/5836 [==============================] - 331s 57ms/step - loss: 0.2437 - auc: 0.9497 - acc: 0.9159 - val_loss: 0.2388 - val_auc: 0.9519 - val_acc: 0.9171\n",
      "Epoch 9/10\n",
      "5836/5836 [==============================] - 337s 58ms/step - loss: 0.2413 - auc: 0.9506 - acc: 0.9168 - val_loss: 0.2426 - val_auc: 0.9516 - val_acc: 0.9161\n",
      "Epoch 10/10\n",
      "5836/5836 [==============================] - 352s 60ms/step - loss: 0.2391 - auc: 0.9513 - acc: 0.9176 - val_loss: 0.2394 - val_auc: 0.9525 - val_acc: 0.9169\n"
     ]
    }
   ],
   "source": [
    "# 9) Train\n",
    "labels = train_labels\n",
    "pos = labels.sum()\n",
    "neg = len(labels) - pos\n",
    "class_weight = {0: 1.0, 1: (neg / (pos + 1e-6))}\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_auc', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "epochs = 20\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, class_weight=class_weight, callbacks=[early_stop, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b728e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649/649 [==============================] - 30s 46ms/step\n",
      "[[160624   5355]\n",
      " [ 22063 143916]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.97      0.92    165979\n",
      "         1.0       0.96      0.87      0.91    165979\n",
      "\n",
      "    accuracy                           0.92    331958\n",
      "   macro avg       0.92      0.92      0.92    331958\n",
      "weighted avg       0.92      0.92      0.92    331958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10) evaluate\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model(\"best_model.keras\", custom_objects={\"TransformerEncoderBlock\": TransformerEncoderBlock})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "threshold = 0.6\n",
    "y_true = val_labels\n",
    "y_pred = (loaded_model.predict(val_ds) > threshold).astype(int).flatten()\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb81e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 211ms/step\n",
      "구매 확률: 0.1012\n"
     ]
    }
   ],
   "source": [
    "# 11) Evaluate and inference\n",
    "# example inference on raw session\n",
    "def predict_session(model, ev_list):\n",
    "    cat_idx, num_vals, mask = session_to_features(ev_list)\n",
    "    feed = {f: np.expand_dims(cat_idx[f], 0) for f in cat_fields}\n",
    "    feed['numeric'] = np.expand_dims(num_vals, 0)\n",
    "    feed['mask'] = np.expand_dims(mask, 0)\n",
    "    prob = model.predict(feed)[0,0]\n",
    "    return prob\n",
    "\n",
    "example_session = [\n",
    "    {\n",
    "        \"event_type\": 2, \n",
    "        \"product_id\": 22200103, \n",
    "        \"category_code\": \"electronics.smartphone\",\n",
    "        \"general_category\": \"electronics\",\n",
    "        \"brand\": \"samsung\",\n",
    "        \"price\": 350.0,\n",
    "        \"relative_price\": 0.45,\n",
    "        \"time_since_start\": 0.0,\n",
    "        \"time_since_prev\": 0.0\n",
    "    },\n",
    "    {\n",
    "        \"event_type\": 2, \n",
    "        \"product_id\": 6902133, \n",
    "        \"category_code\": \"computers.notebook\",\n",
    "        \"general_category\": \"computers\",\n",
    "        \"brand\": \"apple\",\n",
    "        \"price\": 1200.0,\n",
    "        \"relative_price\": 0.9,\n",
    "        \"time_since_start\": 30.0,\n",
    "        \"time_since_prev\": 30.0\n",
    "    }\n",
    "]\n",
    "\n",
    "prob = predict_session(model, example_session)\n",
    "print(f\"구매 확률: {prob:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
